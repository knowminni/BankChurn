# -*- coding: utf-8 -*-
"""BankChurn_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rXUvzF6BYfSvGhzhBlXo97h0XlEPOyO2

# ***Business Problem***
XYZ wants to find out if a customer has churned out or is still associated with the bank.

# ***Data Science Problem***
Build a **Classification Engine** which classifies a customer as Churned Out / Associated on the basis of various features like credit score, balance, tenure, gender, etc.


Since it's a classification problem: We will use ***Logistic Regression***

## ***Step 1: Import Libraries***
"""

#Import Libraries

import pandas as pd
import numpy as np

pd.set_option('display.max_rows', 800)
pd.set_option('display.max_columns', 500)

import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""## ***Step 2: Load Data***


"""

from google.colab import files
uploaded = files.upload()

import io

data = pd.read_csv(io.BytesIO(uploaded['Churn_Modelling.csv']), index_col = 'RowNumber')

data

"""## ***Step 3: Understanding Data: Insights on Shape, Size and Stats***

"""

data.info()

data.describe()

data.head(10)

"""Based on the given dataset, we derive that:

## *Target Variable (Y) : Exited*
## *Independent Variable (X): Features like CreditScore, Gender, Geography etc.*


"""

#Printing numerical and categorical columns in the dataset

num_col = data.select_dtypes(include = np.number).columns
categ_col = data.select_dtypes(exclude = np.number).columns

print('Categorical Columns:\n ', categ_col)
print('************************************************************************************************')
print('Numerical Columns:\n ', num_col)

"""## ***Step 4: Data Preprocessing: Prepare the data for EDA***

"""

#Droping Columns based on Personal Domain Knowledge that will not contribute to the Y Variable

data.drop(['Surname', 'CustomerId'], axis = 1, inplace = True)

#One Hot Encoding of 'Geography' and Label Encoding of 'Gender'
# Geography had values 'France, Germany, Spain' and Gender had values Male/Female

geoDummy = pd.get_dummies(data, prefix = 'Geo', columns = ['Geography'])
genDummy = geoDummy.replace(to_replace = {'Gender': {'Male' : 0, 'Female': 1}})

dataEncoded = genDummy
dataEncoded.head(10)

"""## ***Step 5: Exploratory Data Analysis***

"""

#Check the distribution of Y Variable to check if it is case of an imbalanced dataset

sns.countplot(y = dataEncoded['Exited'], data = dataEncoded)
plt.xlabel('Count of each Target Class')
plt.ylabel('Target Classes')
plt.show()

"""Inference: Very High Class Imbalance since Ratio of Associated : Churned is 8000:2000 which is 4:1"""

#Check Distribution of All Independent Variables (Features)

dataEncoded.hist(figsize=(20,12), bins = 15)
plt.title('Features Distribution')
plt.show()

#MultiCollinearity of Features : Relational Dependencies among the Features

plt.figure(figsize = (18, 12))
sns.heatmap(dataEncoded.corr(), annot = True, cmap = 'seismic', center = 0)

"""Inference: No cases of multi collinearity, All the features can be taken into consideration"""

# Distribution of y variable corresponding to each feature, dependency of y on every x individually

fig, ax = plt.subplots(nrows = 4, ncols = 3, figsize = (25,25))

row = 0
col = 0
for i in range(len(dataEncoded.columns)-1):
    if col > 2:
        row += 1
        col = 0
    axes = ax[row, col]
    sns.boxplot(x = dataEncoded['Exited'], y = dataEncoded[dataEncoded.columns[i]], ax = axes)
    col += 1

plt.tight_layout()
plt.title('Individual Features by Class')
plt.show()

"""## ***Step 6: Model Building***"""

# Splitting Dataset into Y and X series

X = dataEncoded.drop(['Exited'], axis = 1)
y = dataEncoded['Exited']

# Splitting X, y variables into Train and Test sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 105)

#Feature Scaling to bring all the independent variables to the same scale.

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

"""###***Logistic Regression***"""

#Defining Logistic Regression and train the model as per our data

lr = LogisticRegression()
lr.fit(X_train, y_train)

"""## ***Step 6: Prediction and Evaluation of Performance Metrics***"""

# Predict Class
y_pred = lr.predict(X_test)

#Predict Probability of Class
y_predProb = lr.predict_proba(X_test)

"""### ***Conforming Performance Metrics***

#### ***1. Accuracy***
"""

score = lr.score(X_test, y_test)
print("Accuracy Score: ", score)

"""#### ***2.Confusion Matrix***"""

actual = y_test
prediction = y_pred

confMatrix = confusion_matrix(actual, prediction)
print('Confusion Matrix: \n', confMatrix)

"""#### ***Summary of Confusion Matrix:***

The Rows in the confusion matrix are the count of Predicted O's and 1's
The Columns in the confusion matrix are the count of Actual 0's and 1'

Through Confusion Matrix, we inferred that out of a total of 3040 samples of 0's, the model was able to predict 2535 samples of 0's correctly.
Thus 2535/3040 = 83.38% correctly

On the other hand, out of a total 260 samples of 1's, 136 was correctly predicted while 112 was incorrectly predicted. Thus acccuracy for 1's sample = 52.30% only correctly.

This happened because of the High Class Imbalance which needs to be eliminated.

#### ***3.Accuracy, Precision, F1 Score, Recall***
"""

print("Classification Report: \n", classification_report(actual, prediction))

"""### ***Handling Class Imbalance***

#### ***Over Sampling***
"""

from imblearn.over_sampling import RandomOverSampler

X1 = dataEncoded.drop(['Exited'], axis = 1)
y1 = dataEncoded['Exited']

ros = RandomOverSampler()
X_ros, y_ros = ros.fit_sample(X1, y1)

(unique, counts) = np.unique(y_ros, return_counts=True)
frequencies = np.asarray((unique, counts)).T
print(frequencies)

def build_model(X_train, y_train, class_weight = None):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 508)

    #Feature Scaling to bring all the independent variables to the same scale.
    sc = StandardScaler()
    X_train = sc.fit_transform(X_train)
    X_test = sc.fit_transform(X_test)

    lr = LogisticRegression()
    lr.fit(X_train, y_train)

    y_pred = lr.predict(X_test)
    #Predict Probability of Class
    y_predProb = lr.predict_proba(X_test)
    
    actual = y_test
    prediction = y_pred

    print("Confusion Matrix:\n", confusion_matrix(actual, prediction))
    print("Accuracy Score:\n", accuracy_score(actual, prediction))
    print("Classification Report:\n", classification_report(actual, prediction))

    return lr

def evaluateModel(actual, prediction):
    print("Confusion Matrix:\n", confusion_matrix(actual, prediction))
    print("Accuracy Score:\n", accuracy_score(actual, prediction))
    print("Classification Report:\n", classification_report(actual, prediction))

"""#### ***Building New Model that is NOT Facing Class Imbalance***"""

lr_model = build_model(X_ros, y_ros)

# Import Joblib Module from Scikit Learn
from sklearn.externals import joblib
joblib_file = 'Model.h5'
joblib.dump(lr_model, joblib_file)

